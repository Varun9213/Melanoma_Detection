{"cells":[{"metadata":{"id":"KRJeS1g-IjIH","trusted":true},"cell_type":"code","source":"!pip install -q efficientnet >> /dev/null\nimport pandas as pd, numpy as np\nimport tensorflow as tf, re, math\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport zipfile\nfrom kaggle_datasets import KaggleDatasets","execution_count":null,"outputs":[]},{"metadata":{"id":"MtXEZypv9HHX","trusted":true},"cell_type":"code","source":"DEVICE = \"TPU\"\nSEED = 42\nFOLDS = 2\nIMG_SIZES = [256,512]\nINC2019 = [1,1]\nINC2018 = [0,0]\nBATCH_SIZES = [8]*FOLDS\nEPOCHS = [12]*FOLDS\nEFF_NETS = [4,4]\nWGTS = [1/FOLDS]*FOLDS\nTTA = 11","execution_count":null,"outputs":[]},{"metadata":{"id":"ygS1Y7sj-LR2","outputId":"0554c096-d5ad-4408-ee5b-3f11a02e299b","trusted":true},"cell_type":"code","source":"if DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","execution_count":null,"outputs":[]},{"metadata":{"id":"Y0lrQZk4-W6j","outputId":"f1898da9-9cb3-4995-9b36-52e8198be18d","trusted":true},"cell_type":"code","source":"GCS_PATH = [None]*FOLDS; GCS_PATH2 = [None]*FOLDS\nfor i,k in enumerate(IMG_SIZES):\n    GCS_PATH[i] = KaggleDatasets().get_gcs_path('melanoma-%ix%i'%(k,k))\n    GCS_PATH2[i] = KaggleDatasets().get_gcs_path('isic2019-%ix%i'%(k,k))\nfiles_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/train*.tfrec')))\nfiles_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/test*.tfrec')))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input","execution_count":null,"outputs":[]},{"metadata":{"id":"dem9s2dIxyYi","trusted":true},"cell_type":"code","source":"ROT_ = 180\nSHR_ = 2.0\nHZOOM_ = 8.0\nWZOOM_ = 8.0\nHSHIFT_ = 8.0\nWSHIFT_ = 8.0","execution_count":null,"outputs":[]},{"metadata":{"id":"fb_usgUF47KW","trusted":true},"cell_type":"code","source":"def tranformation_matrix(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n\n  def get_3x3(mat):\n    return tf.reshape(tf.concat(mat, axis = 0), [3,3])\n\n  rotation = math.pi * rotation/180\n  shear = math.pi * shear/180\n\n  s1 = tf.reshape(tf.math.sin(rotation),[1])\n  s2 = tf.reshape(tf.math.sin(shear), [1])\n  c1 = tf.reshape(tf.math.cos(rotation),[1])\n  c2 = tf.reshape(tf.math.cos(shear), [1])\n  one = tf.constant([1], dtype=tf.float32)\n  zero = tf.constant([0], dtype=tf.float32)\n\n  rot_mat = get_3x3([c1,s1,zero,-s1,c1,zero,zero,zero,one])\n  zoom_mat = get_3x3([one/height_zoom, zero, zero, zero,one/width_zoom, zero,zero,zero,one])\n  shear_mat = get_3x3([one, s2, zero, zero, c2, zero, zero, zero, one])\n  shift_mat = get_3x3([one,zero,height_shift, zero,one,width_shift,zero,zero,one])\n\n  return K.dot(K.dot(rot_mat, zoom_mat), K.dot(shear_mat, shift_mat))\n\n\ndef transform(img, dim):\n  \n  rot = ROT_ * tf.random.normal([1], dtype='float32')\n  shr = SHR_ * tf.random.normal([1], dtype='float32') \n  h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n  w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n  h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n  w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32')\n\n  m = tranformation_matrix(rot, shr, h_zoom, w_zoom, h_shift, w_shift)\n\n  x = tf.repeat(tf.range(dim//2, -dim//2, -1), dim)\n  y = tf.tile(tf.range(dim//2, -dim//2, -1), [dim])\n  z = tf.ones([dim*dim], dtype=tf.int32)\n  indx = tf.stack([x,y,z])\n\n  indx_t = K.dot(m, tf.cast(indx, dtype=\"float32\"))\n  indx_t = K.cast(tf.math.round(indx_t), dtype=\"int32\")\n  indx_t = K.clip(indx_t, -dim//2+dim%2+1, dim//2)\n\n\n  indx_f = tf.stack([dim//2-indx_t[0,], dim//2-indx_t[1,]])\n  d = tf.reshape(tf.gather_nd(img, tf.transpose(indx_f)), [dim,dim,3])\n\n  return d","execution_count":null,"outputs":[]},{"metadata":{"id":"PfRy7uD6Au-L","trusted":true},"cell_type":"code","source":"def read_labeled_data(ex):\n\n  format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n        }\n\n  ex = tf.io.parse_single_example(ex, format)\n  return ex[\"image\"], ex[\"target\"]\n\ndef read_unlabeled_data(ex, return_image_name):\n\n  format = {\n      'image'        : tf.io.FixedLenFeature([], tf.string),\n      'image_name'   : tf.io.FixedLenFeature([], tf.string)\n  }\n\n  ex = tf.io.parse_single_example(ex, format)\n  return ex[\"image\"], ex[\"image_name\"] if return_image_name else 0\n\ndef prepare_image(img, augment = True, dim = 256):\n\n  img = tf.image.decode_jpeg(img, 3)\n  img = tf.cast(img, tf.float32) / 255.0\n\n  if augment:\n\n    img = transform(img, dim)\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.random_saturation(img, 0.7, 1.3)\n    img = tf.image.random_brightness(img, 0.1)\n    img = tf.image.random_contrast(img, 0.8, 1.2)\n\n  img = tf.reshape(img, [dim,dim, 3])\n\n  return img\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n         for filename in filenames]\n    return np.sum(n)","execution_count":null,"outputs":[]},{"metadata":{"id":"HUfDNBM7Avhm","trusted":true},"cell_type":"code","source":"def get_dataset(files, shuffle = False, augment = False, repeat = False, labeled = True, return_image_names = True, batch_size = 16, dim=256):\n  \n  ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n  ds = ds.cache()\n\n  if repeat:\n    ds = ds.repeat()\n\n  if shuffle:\n    ds = ds.shuffle(256*16)\n    opt = tf.data.Options()\n    opt.experimental_deterministic = False\n    ds = ds.with_options(opt)\n\n  if labeled:\n    ds = ds.map(read_labeled_data, num_parallel_calls = AUTO)\n  else:\n    ds = ds.map(lambda ex: read_unlabeled_data(ex, return_image_names), num_parallel_calls = AUTO)\n\n  ds = ds.map(lambda image, image_name_or_label: (prepare_image(image,augment,dim), image_name_or_label), num_parallel_calls=AUTO)\n  ds = ds.batch(batch_size * REPLICAS)\n  ds = ds.prefetch(AUTO)\n  return ds\n","execution_count":null,"outputs":[]},{"metadata":{"id":"KPMUT-FT2W0c","trusted":true},"cell_type":"code","source":"EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6]\n\ndef build_model(dim=128, ef=0):\n    inp = tf.keras.layers.Input(shape=(dim,dim,3))\n    base = EFNS[ef](input_shape=(dim,dim,3),weights='imagenet',include_top=False)\n    x = base(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=inp,outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"NyRrfARzGrKl","trusted":true},"cell_type":"code","source":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * REPLICAS * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","execution_count":null,"outputs":[]},{"metadata":{"id":"jZSMBPVjGvSK","outputId":"a02cd349-9749-43c4-da48-45ac9c3d466a","trusted":true},"cell_type":"code","source":"# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\nVERBOSE = 1\nDISPLAY_PLOT = True\n\nskf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\noof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [] \npreds = np.zeros((count_data_items(files_test),1))\n\nfor fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))):\n    \n    # DISPLAY FOLD INFO\n    if DEVICE=='TPU':\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    print('#'*25); print('#### FOLD',fold+1)\n    print('#### Image Size %i with EfficientNet B%i and batch_size %i'%\n          (IMG_SIZES[fold],EFF_NETS[fold],BATCH_SIZES[fold]*REPLICAS))\n    \n    # CREATE TRAIN AND VALIDATION SUBSETS\n    files_train = tf.io.gfile.glob([GCS_PATH[fold] + '/train%.2i*.tfrec'%x for x in idxT])\n    if INC2019[fold]:\n        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + '/train%.2i*.tfrec'%x for x in idxT*2+1])\n        print('#### Using 2019 external data')\n    if INC2018[fold]:\n        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + '/train%.2i*.tfrec'%x for x in idxT*2])\n        print('#### Using 2018+2017 external data')\n    np.random.shuffle(files_train); print('#'*25)\n    files_valid = tf.io.gfile.glob([GCS_PATH[fold] + '/train%.2i*.tfrec'%x for x in idxV])\n    files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + '/test*.tfrec')))\n    \n    # BUILD MODEL\n    K.clear_session()\n    with strategy.scope():\n        model = build_model(dim=IMG_SIZES[fold],ef=EFF_NETS[fold])\n        \n    # SAVE BEST MODEL EACH FOLD\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='min', save_freq='epoch')\n   \n    # TRAIN\n    print('Training...')\n    history = model.fit(\n        get_dataset(files_train, augment=True, shuffle=True, repeat=True,\n                dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), \n        epochs=EPOCHS[fold], callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])], \n        steps_per_epoch=count_data_items(files_train)/BATCH_SIZES[fold]//REPLICAS,\n        validation_data=get_dataset(files_valid,augment=False,shuffle=False,\n                repeat=False,dim=IMG_SIZES[fold]), #class_weight = {0:1,1:2},\n        verbose=VERBOSE\n    )\n    \n    print('Loading best model...')\n    model.load_weights('fold-%i.h5'%fold)\n    \n    # PREDICT OOF USING TTA\n    print('Predicting OOF with TTA...')\n    ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ct_valid = count_data_items(files_valid); STEPS = TTA * ct_valid/BATCH_SIZES[fold]/4/REPLICAS\n    pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,] \n    oof_pred.append( np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1) )                 \n    #oof_pred.append(model.predict(get_dataset(files_valid,dim=IMG_SIZES[fold]),verbose=1))\n    \n    # GET OOF TARGETS AND NAMES\n    ds_valid = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n            labeled=True, return_image_names=True)\n    oof_tar.append( np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) )\n    oof_folds.append( np.ones_like(oof_tar[-1],dtype='int8')*fold )\n    ds = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n                labeled=False, return_image_names=True)\n    oof_names.append( np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())]))\n    \n    # PREDICT TEST USING TTA\n    print('Predicting Test with TTA...')\n    ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ct_test = count_data_items(files_test); STEPS = TTA * ct_test/BATCH_SIZES[fold]/4/REPLICAS\n    pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,] \n    preds[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1) * WGTS[fold]\n    \n    # REPORT RESULTS\n    auc = roc_auc_score(oof_tar[-1],oof_pred[-1])\n    oof_val.append(np.max( history.history['val_auc'] ))\n    print('#### FOLD %i OOF AUC without TTA = %.3f, with TTA = %.3f'%(fold+1,oof_val[-1],auc))\n    \n    # PLOT TRAINING\n    if DISPLAY_PLOT:\n        plt.figure(figsize=(15,5))\n        plt.plot(np.arange(EPOCHS[fold]),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n        plt.plot(np.arange(EPOCHS[fold]),history.history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n        x = np.argmax( history.history['val_auc'] ); y = np.max( history.history['val_auc'] )\n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n        plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(np.arange(EPOCHS[fold]),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n        plt2.plot(np.arange(EPOCHS[fold]),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n        x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n        plt.ylabel('Loss',size=14)\n        plt.title('FOLD %i - Image Size %i, EfficientNet B%i, inc2019=%i, inc2018=%i'%\n                (fold+1,IMG_SIZES[fold],EFF_NETS[fold],INC2019[fold],INC2018[fold]),size=18)\n        plt.legend(loc=3)\n        plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"id":"vkq62V1dJ09e","trusted":true},"cell_type":"code","source":"# COMPUTE OVERALL OOF AUC\noof = np.concatenate(oof_pred); true = np.concatenate(oof_tar);\nnames = np.concatenate(oof_names); folds = np.concatenate(oof_folds)\nauc = roc_auc_score(true,oof)\nprint('Overall OOF AUC with TTA = %.3f'%auc)\n\n# SAVE OOF TO DISK\ndf_oof = pd.DataFrame(dict(\n    image_name = names, target=true, pred = oof, fold=folds))\ndf_oof.to_csv('./oof.csv',index=False)\ndf_oof.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}